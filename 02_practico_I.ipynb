{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h4>Universidad Nacional de Córdoba - Facultad de Matemática, Astronomía, Física y Computación</h4>\n",
    "<h3>Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</h3>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Práctico I - Estadística </h1>\n",
    "<h3> Análisis y Visualización de Datos - 2019 </h3>\n",
    "\n",
    "Durante este práctico vamos a trabajar sobre el dataset [Human Freedom Index 2018](https://www.cato.org/human-freedom-index-new) de el instituto Cato. Este índice mide en detalle lo que entendemos como libertad, utilizando 79 indicadores de libertad personal y económica en distintos aspectos, hasta obtener un hermoso numerito del 1 al 10. Usaremos una [versión ya limpia del dataset](https://www.kaggle.com/gsutters/the-human-freedom-index/home) que pueden descargar desde Kaggle.\n",
    "\n",
    "Las variables más importantes sobre las que trabaja el dataset son:\n",
    "\n",
    "* Rule of Law\n",
    "* Security and Safety\n",
    "* Movement\n",
    "* Religion\n",
    "* Association, Assembly, and Civil Society\n",
    "* Expression and Information\n",
    "* Identity and Relationships\n",
    "* Size of Government\n",
    "* Legal System and Property Rights\n",
    "* Access to Sound Money\n",
    "* Freedom to Trade Internationally\n",
    "* Regulation of Credit, Labor, and Business\n",
    "\n",
    "Nosotros centrarermos nuestro análisis en variables relacionadas a *Identity and Relationships* en paises de Latinoamérica, y los compararemos con las estadísticas globales. La pregunta a responder es simple: **¿Qué niveles de libertad se viven en Latinoamérica, especificamente en cuanto libertades de indentidad?**. Sin embargo, para hacer un análisis de los datos tenemos que platear también estas sub preguntas:\n",
    "\n",
    "1. ¿Qué significa tener un puntaje de 4.5? Hay que poner los puntajes de la región en contexto con los datos del resto del mundo.\n",
    "2. ¿Cuál es la tendencia a lo largo de los años? ¿Estamos mejorando, empeorando?\n",
    "3. En este estudio, la libertad se mide con dos estimadores principales: *hf_score* que hace referencia a Human Freedom, y *ef_score* que hace referencia a Economic Freedom. Estos dos estimadores, ¿se relacionan de la misma manera con la libertad de identidad?\n",
    "\n",
    "Inicialmente, en toda exploración de datos tenemos muy poca información a priori sobre el significado de los datos y tenemos que empezar por comprenderlos. Les proponemos los siguientes ejercicios como guía para comenzar esta exploración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas\n",
    "import seaborn\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.9.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seaborn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1458, 123)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.read_csv('hfi_cc_2018.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'ISO_code', 'countries', 'region', 'pf_rol_procedural',\n",
       "       'pf_rol_civil', 'pf_rol_criminal', 'pf_rol', 'pf_ss_homicide',\n",
       "       'pf_ss_disappearances_disap',\n",
       "       ...\n",
       "       'ef_regulation_business_bribes', 'ef_regulation_business_licensing',\n",
       "       'ef_regulation_business_compliance', 'ef_regulation_business',\n",
       "       'ef_regulation', 'ef_score', 'ef_rank', 'hf_score', 'hf_rank',\n",
       "       'hf_quartile'],\n",
       "      dtype='object', length=123)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns  # Way too many columns!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por suerte las columnas tienen un prefijo que nos ayuda a identificar a qué sección pertenecen. Nos quedamos sólo con las que comienzan con *pf_indentity*, junto con otras columnas más generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_cols = ['year', 'ISO_code', 'countries', 'region']\n",
    "\n",
    "important_cols += [col for col in dataset.columns if 'pf_identity' in col]\n",
    "\n",
    "important_cols += [\n",
    "    'ef_score', # Economic Freedom (score)\n",
    "    'ef_rank', # Economic Freedom (rank)\n",
    "    'hf_score', # Human Freedom (score)\n",
    "    'hf_rank', # Human Freedom (rank)\n",
    "]\n",
    "\n",
    "# Imprime dataset con las columnas requeridas\n",
    "#dataset[important_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estadísticos descriptivos\n",
    "\n",
    "  1. Para comenzar con un pantallazo de los datos, calcular el rango de las variables.\n",
    "  2. Obtener media, mediana y desviación estándar de las variables *pf_identity* y *hf_score* en el mundo y compararla con la de Latinoamérica y el caribe. ¿Tiene sentido calcular la moda? \n",
    "  3. ¿Son todos los valores de *pf_identity*  y *hf_score* directamente comparables? ¿Qué otra variable podría influenciarlos?\n",
    "  4. ¿Cómo pueden sanearse los valores faltantes?\n",
    "  5. ¿Encuentra outliers en estas dos variables? ¿Qué método utiliza para detectarlos? ¿Los outliers, son globales o por grupo? ¿Los eliminaría del conjunto de datos?\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0   Variables Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_latin_america = dataset[dataset.region == \"Latin America & the Caribbean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1   Rango de Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Definición de los parametros a examinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parametros = []\n",
    "parametros += [col for col in dataset.columns if 'pf_identity' in col]\n",
    "parametros += [\n",
    "    'ef_score', # Economic Freedom (score)\n",
    "    'ef_rank', # Economic Freedom (rank)\n",
    "    'hf_score', # Human Freedom (score)\n",
    "    'hf_rank', # Human Freedom (rank)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Minimo y Máximo de los parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Mínimo</th>\n",
       "      <th>Máximo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pf_identity_legal</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pf_identity_parental_marriage</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pf_identity_parental_divorce</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pf_identity_parental</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pf_identity_sex_male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pf_identity_sex_female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>pf_identity_sex</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pf_identity_divorce</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pf_identity</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ef_score</td>\n",
       "      <td>2.880000</td>\n",
       "      <td>9.190000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ef_rank</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hf_score</td>\n",
       "      <td>3.765827</td>\n",
       "      <td>9.126313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hf_rank</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>162.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Variable    Mínimo      Máximo\n",
       "0               pf_identity_legal  0.000000   10.000000\n",
       "1   pf_identity_parental_marriage  0.000000   10.000000\n",
       "2    pf_identity_parental_divorce  0.000000   10.000000\n",
       "3            pf_identity_parental  0.000000   10.000000\n",
       "4            pf_identity_sex_male  0.000000   10.000000\n",
       "5          pf_identity_sex_female  0.000000   10.000000\n",
       "6                 pf_identity_sex  0.000000   10.000000\n",
       "7             pf_identity_divorce  0.000000   10.000000\n",
       "8                     pf_identity  0.000000   10.000000\n",
       "9                        ef_score  2.880000    9.190000\n",
       "10                        ef_rank  1.000000  162.000000\n",
       "11                       hf_score  3.765827    9.126313\n",
       "12                        hf_rank  1.000000  162.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datos = [(x, dataset[x].min(), dataset[x].max()) for x in dataset[parametros]]\n",
    "df    = pandas.DataFrame(data=datos, columns=['Variable','Mínimo','Máximo'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Analisis de los parametros a examinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_identity_legal\n",
      "\n",
      "pf_identity_legal: Minimo = 0.0\n",
      "pf_identity_legal: Maximo = 10.0\n",
      "Datos : [ 0. nan 10.  7.]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_parental_marriage\n",
      "\n",
      "pf_identity_parental_marriage: Minimo = 0.0\n",
      "pf_identity_parental_marriage: Maximo = 10.0\n",
      "Datos : [10.  0. nan  5.]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_parental_divorce\n",
      "\n",
      "pf_identity_parental_divorce: Minimo = 0.0\n",
      "pf_identity_parental_divorce: Maximo = 10.0\n",
      "Datos : [10.  5.  0. nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_parental\n",
      "\n",
      "pf_identity_parental: Minimo = 0.0\n",
      "pf_identity_parental: Maximo = 10.0\n",
      "Datos : [10.   2.5  0.   5.   7.5  nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_sex_male\n",
      "\n",
      "pf_identity_sex_male: Minimo = 0.0\n",
      "pf_identity_sex_male: Maximo = 10.0\n",
      "Datos : [10.  0.  5. nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_sex_female\n",
      "\n",
      "pf_identity_sex_female: Minimo = 0.0\n",
      "pf_identity_sex_female: Maximo = 10.0\n",
      "Datos : [10.  0.  5. nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_sex\n",
      "\n",
      "pf_identity_sex: Minimo = 0.0\n",
      "pf_identity_sex: Maximo = 10.0\n",
      "Datos : [10.  0.  5. nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity_divorce\n",
      "\n",
      "pf_identity_divorce: Minimo = 0.0\n",
      "pf_identity_divorce: Maximo = 10.0\n",
      "Datos : [ 5.  0. 10. nan]\n",
      "--------------------------\n",
      "\n",
      "pf_identity\n",
      "\n",
      "pf_identity: Minimo = 0.0\n",
      "pf_identity: Maximo = 10.0\n",
      "Datos : [ 6.25        0.83333333  7.5        10.          8.          9.25\n",
      "  3.33333333  1.25        5.          3.75        4.25        1.66666667\n",
      "  5.83333333  8.33333333  9.16666667  6.875       0.          6.66666667\n",
      "  6.75        5.5         3.          5.625       4.16666667  9.\n",
      "  4.375       9.375       2.5         1.75        4.875       8.75\n",
      "         nan]\n",
      "--------------------------\n",
      "\n",
      "ef_score\n",
      "\n",
      "ef_score: Minimo = 2.88\n",
      "ef_score: Maximo = 9.19\n",
      "Datos : [7.54 4.99 5.17 4.84 7.57 7.98 7.58 6.49 7.34 7.56 6.3  6.43 6.23 7.32\n",
      " 6.86 5.98 7.02 6.6  7.43 5.75 6.93 7.41 6.05 5.92 7.17 5.82 6.68 5.11\n",
      " 5.44 7.8  6.46 6.5  5.67 5.02 7.55 6.   6.96 7.71 7.77 7.18 6.06 5.72\n",
      " 7.15 7.86 5.73 6.79 7.65 7.25 5.84 8.02 7.69 7.64 5.93 5.25 6.31 6.51\n",
      " 7.06 8.97 7.22 6.63 7.16 6.03 5.4  8.07 7.49 7.27 7.47 7.46 7.11 7.2\n",
      " 7.53 6.75 6.91 6.99 6.38 6.56 4.74 7.6  7.13 6.19 5.86 6.92 5.8  7.73\n",
      " 5.96 8.01 6.9  6.64 7.4  7.04 6.37 5.5  5.42 6.4  6.53 8.49 6.01 6.32\n",
      " 6.76 6.25 6.95 7.51 6.83 7.48 6.52 6.22 6.85 7.23 8.84 7.3  7.05 6.65\n",
      " 5.36 6.36 7.44 8.39 7.89 6.72 6.55 6.73 6.29 6.84 7.5  8.   8.03 2.88\n",
      " 6.42 6.34 6.61 4.83 4.98 7.59 7.35 7.37 6.33  nan 6.81 5.78 6.77 5.87\n",
      " 5.79 7.91 6.67 4.63 5.23 7.79 6.39 4.81 7.75 7.24 6.09 5.77 7.14 7.93\n",
      " 5.55 6.82 7.67 6.45 5.74 5.58 6.54 7.19 7.   8.25 7.31 7.26 6.59 6.97\n",
      " 7.72 7.01 6.87 4.89 5.76 5.7  6.88 6.57 7.38 6.74 5.63 5.32 8.48 6.17\n",
      " 7.36 7.7  6.24 6.15 7.33 7.08 6.44 8.37 4.96 7.76 6.8  5.69 6.48 7.45\n",
      " 2.96 6.07 5.01 5.13 4.49 7.68 5.94 6.21 6.78 6.89 6.02 5.97 5.9  8.22\n",
      " 5.16 4.97 7.84 6.58 4.69 7.81 5.66 5.46 5.57 6.41 5.62 5.35 9.   8.12\n",
      " 7.52 7.1  6.98 6.47 7.78 5.81 5.68 6.28 7.74 7.62 5.89 6.71 8.31 6.62\n",
      " 5.91 3.28 5.04 4.88 7.92 7.63 6.2  7.03 5.95 5.71 7.88 4.55 7.07 5.88\n",
      " 5.53 7.87 7.66 5.65 5.07 6.04 8.96 8.04 7.42 7.12 4.78 7.61 5.6  6.14\n",
      " 6.12 8.64 4.07 6.35 7.96 3.32 5.3  4.95 5.28 5.22 7.99 4.93 4.72 5.2\n",
      " 4.39 5.64 7.09 5.37 6.94 6.18 5.99 8.15 5.   8.47 6.27 8.2  5.27 6.69\n",
      " 5.33 7.28 3.82 5.06 6.7  7.95 4.48 8.99 7.83 8.05 4.44 8.76 8.23 5.45\n",
      " 7.9  7.39 3.91 5.59 5.18 6.08 6.26 5.38 7.21 7.82 5.54 5.05 9.02 5.51\n",
      " 4.06 8.33 8.3  3.96 4.46 5.14 7.97 7.29 6.1  5.43 5.85 4.9  5.19 9.09\n",
      " 7.85 6.66 5.47 4.22 8.27 8.77 8.26 8.08 4.36 4.5  4.41 9.19 3.86 8.4\n",
      " 5.39 8.92 8.24 8.17]\n",
      "--------------------------\n",
      "\n",
      "ef_rank\n",
      "\n",
      "ef_rank: Minimo = 1.0\n",
      "ef_rank: Maximo = 162.0\n",
      "Datos : [ 34. 159. 155. 160.  29.  10.  27. 106.  49.  30. 120. 111. 123.  52.\n",
      "  83. 134.  73.  98.  44. 144.  77.  46. 129. 138.  64. 141.  93. 156.\n",
      " 150.  15. 108. 105. 148. 157.  33. 132.  75.  18.  16.  62. 127. 147.\n",
      "  67.  13. 146.  88.  22.  57. 140.   7.  20.  23. 137. 154. 119. 104.\n",
      "  70.   1.  59.  96.  65. 130. 152.   5.  38.  54.  41.  42.  69.  61.\n",
      "  35.  90.  81.  74. 114. 100. 161.  25.  68. 126. 139.  79. 143.  17.\n",
      " 136.   8.  82.  95.  48.  72. 115. 149. 151. 113. 102.   3. 131. 118.\n",
      "  89. 122.  76.  36.  87.  40. 103. 125.  84.  58.   2.  53.  71.  94.\n",
      " 153. 116.  43.   4.  12.  92. 101.  91. 121.  86.  37.   9.   6. 162.\n",
      " 112. 117.  97.  50. 107.  nan  85. 133. 158.  14. 124.  63.  24.  55.\n",
      "  32. 109.  80.  66.  47. 145.  78.  21.  31. 110.  56. 128. 142.  26.\n",
      "  19.  99.  28. 135.  45.  60.  51.  11.  39.]\n",
      "--------------------------\n",
      "\n",
      "hf_score\n",
      "\n",
      "hf_score: Minimo = 3.765826877\n",
      "hf_score: Maximo = 9.126312939\n",
      "Datos : [7.56814029 5.13588606 5.64066218 ... 6.42020631 6.64772441 4.93557073]\n",
      "--------------------------\n",
      "\n",
      "hf_rank\n",
      "\n",
      "hf_rank: Minimo = 1.0\n",
      "hf_rank: Maximo = 162.0\n",
      "Datos : [ 48. 155. 142. 107.  57.   4.  16. 130.  50.  75. 138.  67. 128.  27.\n",
      "  64.  90.  83.  92.  59. 123.  41.  88. 154.  61. 144.   5.  53. 152.\n",
      " 147.  32. 135.  87. 151. 136.  37. 100.  44.  30.  21.   6.  69. 156.\n",
      "  70.  14. 150.  55.  10. 115.  40.  13.  66. 141. 133.  96.   3.  42.\n",
      " 110.  85. 153. 159.   8.  49.  34.  60.  31.  82. 124.  99. 113.  23.\n",
      "  95. 106. 158.  20.  15.  56. 103.  97. 134.  19.  45.  52. 127. 149.\n",
      "  74.   1. 137. 132. 129. 140.  46.  72.  47.  73.  39.  22.  24. 119.\n",
      "  71. 146. 105.  51.  54. 112.  25.  36.  35.  63. 157. 122.  17.   2.\n",
      " 162. 102.  98. 114.  81. 109. 118. 117.  43. 161. 160. 116. 143.  12.\n",
      " 131.  nan  94.  76.  58.  86. 111.  28.  65. 148.  68. 120.  77.  79.\n",
      "  62.  80.  89. 101.  18.  38. 145.   7. 139.  26.  33.  84.   9. 126.\n",
      " 121.  93. 104.  91.  11.  29. 125.  78. 108.]\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in dataset[parametros] : \n",
    "    max = dataset[x].max()\n",
    "    min = dataset[x].min()\n",
    "    print(x + \"\\n\")\n",
    "    print(x + \": Minimo = \" + str(min))\n",
    "    print(x + \": Maximo = \" + str(max))\n",
    "    print(\"Datos : \" + str(dataset[x].unique()) + \"\\n\" + \"--------------------------\" + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Rango de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Variable</th>\n",
    "        <th>Rango en Notación de conjunto</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_legal</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_parental_marriage</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_parental_divorce</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_parental</td>\n",
    "        <td>{ x pertenece a Q | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_sex_male</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_sex_female</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_sex</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity_divorce</td>\n",
    "        <td>{ x pertenece a N | 0 <= x <= 10}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>pf_identity</td>\n",
    "        <td>{ x pertenece a R | 0 <= x <= 10}  Conjunto infinito no contable</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>ef_score</td>\n",
    "        <td>{ x pertenece a Q | 2.88 <= x <= 9.19}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>ef_rank</td>\n",
    "        <td>{ x pertenece a N | 1 <= x <= 162}  Conjunto finito</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>hf_score</td>\n",
    "        <td>{ x pertenece a R | 0 <= x <= 10}  Conjunto infinito no contable</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>hf_rank</td>\n",
    "        <td>{ x pertenece a N | 1 <= x <= 162}  Conjunto finito</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2   Media, mediana y desviación estándar de las variables pf_identity y hf_score en el mundo y compararla con la de Latinoamérica y el caribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lugar</th>\n",
       "      <th>Media</th>\n",
       "      <th>Mediana</th>\n",
       "      <th>Desviación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mundo</td>\n",
       "      <td>7.334180</td>\n",
       "      <td>9.166667</td>\n",
       "      <td>3.159473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ámerica Latina</td>\n",
       "      <td>8.765086</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.711732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lugar     Media    Mediana  Desviación\n",
       "0           Mundo  7.334180   9.166667    3.159473\n",
       "1  Ámerica Latina  8.765086  10.000000    1.711732"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Datos Mundiales\n",
    "pf_identity_mundo_media              = dataset['pf_identity'].mean()\n",
    "pf_identity_mundo_mediana            = dataset['pf_identity'].median()\n",
    "pf_identity_mundo_desviacion         = dataset['pf_identity'].std()\n",
    "\n",
    "# Datos para Ámerica Latina\n",
    "pf_identity_latin_america_media      = dataset_latin_america['pf_identity'].mean()\n",
    "pf_identity_latin_america_mediana    = dataset_latin_america['pf_identity'].median()\n",
    "pf_identity_latin_america_desviacion = dataset_latin_america['pf_identity'].std()\n",
    "\n",
    "# Datos comparativos\n",
    "\n",
    "datos_comparativos = [['Mundo',pf_identity_mundo_media, pf_identity_mundo_mediana, pf_identity_mundo_desviacion],\n",
    "                      ['Ámerica Latina',pf_identity_latin_america_media, pf_identity_latin_america_mediana, pf_identity_latin_america_desviacion]\n",
    "                     ]\n",
    "\n",
    "cuadro             = pandas.DataFrame(datos_comparativos, columns=['Lugar','Media', 'Mediana', 'Desviación'])\n",
    "display(cuadro)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de la comparación\n",
    "\n",
    "Deacuerdo a estos datos el indice de pf_identity en latinoamerica es más alto que el Mundial, con una desviación menor (que indica la similitud de los paises de Ámerica Latina y el Caribe) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Tiene sentido calcular la moda?\n",
    "\n",
    "No tiene sentido calcular la moda, dado que los datos no son discretos, son variables continuas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Agregación de datos\n",
    "\n",
    "1. Grafiquen la media de la variable *pf_identity* y *hf_score* a través de los años.\n",
    "2. Realicen los mismos gráficos, pero separando por regiones (Cada variable en un gráfico distinto, sino no se ve nada). ¿La tendencia observada, es la misma que si no dividimos por regiones?\n",
    "3. Si lo consideran necesario, grafiquen algunos países de Latinoamerica para tratar de explicar la tendencia de la variable *pf_identity* en la región. ¿Cómo seleccionarion los países relevantes a esa tendencia?\n",
    "\n",
    "Hint: hay un gráfico de seaborn que hace todo por vos!\n",
    "\n",
    "Sólo por curiosidad, graficar la tendencia de *hf_score* y *ef_score* a través de los años. ¿Tienen alguna hipótesis para este comportamiento?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico de la media de pf_identity y hf_score a lo largo del tiempo para la región de Latinoamérica y Caribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#america latina\n",
    "\n",
    "dataset_imp_cols = dataset[important_cols]\n",
    "latin_american=dataset_imp_cols[dataset_imp_cols.region == 'Latin America & the Caribbean']\n",
    "\n",
    "years = list(latin_american.year.unique())\n",
    "years.reverse()\n",
    "\n",
    "pf_identity_med = []\n",
    "hf_score_med = []\n",
    "\n",
    "for y in years:\n",
    "    pf_identity_med.append(\n",
    "        latin_american[latin_american.year == y].pf_identity.mean() \n",
    "    )\n",
    "    hf_score_med.append(\n",
    "        latin_american[latin_american.year == y].hf_score.mean() \n",
    "    )\n",
    "\n",
    "df1 = pandas.DataFrame(dict(years=years, pf_identity_media=pf_identity_med))\n",
    "df2 = pandas.DataFrame(dict(years=years, hf_score_media=hf_score_med))\n",
    "\n",
    "\n",
    "seaborn.lineplot(x='years', y='pf_identity_media', data=df1)\n",
    "plt.figure()\n",
    "seaborn.lineplot(x='years', y='hf_score_media', data=df2, color='red')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "seaborn.regplot(x='years', y='pf_identity_media', data=df1, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "seaborn.regplot(x='years', y='hf_score_media', data=df2, ax=ax2, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafico de la media de pf_identity y hf_score a lo largo de los años a nivel global, salvo región Latinoamérica y Caribe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resto del mundo menos america latina y caribe\n",
    "\n",
    "dataset_imp_cols = dataset[important_cols]\n",
    "latin_american=dataset_imp_cols[dataset_imp_cols.region != 'Latin America & the Caribbean']\n",
    "\n",
    "years = list(latin_american.year.unique())\n",
    "years.reverse()\n",
    "\n",
    "pf_identity_med = []\n",
    "hf_score_med = []\n",
    "\n",
    "for y in years:\n",
    "    pf_identity_med.append(\n",
    "        latin_american[latin_american.year == y].pf_identity.mean() \n",
    "    )\n",
    "    hf_score_med.append(\n",
    "        latin_american[latin_american.year == y].hf_score.mean() \n",
    "    )\n",
    "\n",
    "df1 = pandas.DataFrame(dict(years=years, pf_identity_media=pf_identity_med))\n",
    "df2 = pandas.DataFrame(dict(years=years, hf_score_media=hf_score_med))\n",
    "\n",
    "\n",
    "seaborn.lineplot(x='years', y='pf_identity_media', data=df1)\n",
    "plt.figure()\n",
    "seaborn.lineplot(x='years', y='hf_score_media', data=df2, color='red')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "seaborn.regplot(x='years', y='pf_identity_media', data=df1, ax=ax)\n",
    "ax2 = ax.twinx()\n",
    "seaborn.regplot(x='years', y='hf_score_media', data=df2, ax=ax2, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Distribuciones\n",
    "  1. Graficar en un mismo histograma la distribución de la variable *pf_identity* en global, y en Latinoamérica y el caribe. Repetir para la variable *hf_score*. ¿Visualmente, a qué tipo de distribución corresponde cada variable? ¿Es correcto utilizar todos los registros para esas zonas en estos gráficos?\n",
    "  2. Realizar una prueba de Kolmogorov-Smirnof para comprobar analíticamente si estas variables responden la distribución propuesta en el ejercicio anterior. Hint: podés usar https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.kstest.html, pero hay que tener en cuenta que si la distribución es \"norm\", entonces va a comparar los datos con una distribución normal con media 0 y desviación estándar 1. Se puede utilizar la distribución sobre todos los datos o sólo sobre Latinoamérica.\n",
    "  3. Realizar un gráfico QQ de las mismas distribuciones. Se puede utilizar a,bas distribuciones sobre todos los datos o sólo sobre Latinoamérica, pero no cruzadas.\n",
    "  4. Medir la asimetría (skew) y curtosis de las mismas distribuciones con las que realizó el gráfico anterior. ¿Cómo se relacionan estos estadísticos con la forma del gráfico QQ obtenido previamente? ¿El gráfico QQ provee más información que no esté presente en estos estadísticos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 histograma la distribución de la variable pf_identity en global, y en Latinoamérica y el caribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "seaborn.distplot(ds['pf_identity'].dropna(), label='Global')\n",
    "seaborn.distplot(ds[ds.region == \"Latin America & the Caribbean\"]['pf_identity'].dropna(), label='Latin America')\n",
    "plt.ylabel('Densidad de probabilidad')\n",
    "plt.legend()\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis <br>\n",
    "<ul>\n",
    "    <li>America Latina tiene un indice (de probabilidad) más alto, en libertad de identidad comparado con el mundo</li>\n",
    "    <li>El tipo de distribución es Normal con cola a la izquiera y no simetrica, para ambos casos (Mundo y America Latina)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 histograma de distribución de la variable hf_score en global, y en Latinoamérica y el caribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "seaborn.distplot(ds['hf_score'].dropna(), label='Global')\n",
    "seaborn.distplot(ds[ds.region == \"Latin America & the Caribbean\"]['hf_score'].dropna(), label='Latin America')\n",
    "plt.ylabel('Densidad de probabilidad')\n",
    "plt.legend()\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis\n",
    "<ul>\n",
    "    <li>America Latina tiene un indice (de probabilidad) más alto, en libertades humanas (human freedom score) comparado con el mundo</li>\n",
    "    <li>El tipo de distribución es Normal no simetrica, para ambos casos (Mundo y America Latina)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Realizar una prueba de Kolmogorov-Smirnof para comprobar analíticamente si estas variables responden la distribución propuesta en el ejercicio anterior.  \n",
    "\n",
    "pero hay que tener en cuenta que si la distribución es \"norm\", entonces va a comparar los datos con una distribución normal con media 0 y desviación estándar 1. Se puede utilizar la distribución sobre todos los datos o sólo sobre Latinoamérica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### pf_ identity en el mundo y America Latina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(dataset['pf_identity'].dropna(), 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(dataset[dataset['region'] == 'Latin America & the Caribbean']['pf_identity'].dropna(), 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### hf_score en el mundo y America Latina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(dataset['hf_score'].dropna(), 'norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kstest(dataset[dataset['region'] == 'Latin America & the Caribbean']['hf_score'].dropna(), 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisis\n",
    "<ul>\n",
    "    <li>En ambos casos (pf_identity y hf_score) al optener pvalue = 0, no podemos afirmar que la distribución sea normal </li>\n",
    "    <li>Dado que el test dice que la distribución no es normal, las distribuciones del punto 3.1.1 y 3.1.2 posiblemente no lo sean </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Gráfico QQ de las distribuciones de pf_identity en el mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero sacamos los parametros de la media y desviación estándar (de pf_identity en el mundo), para luego hacer una muestra aleatoria que si tenga una distribución normal que vamos a usar como comparador en el gráfico QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Definimos la variable para el dataset que vamos a utilizar\n",
    "dataset_pf_identity_mundo = dataset['pf_identity'].dropna()\n",
    "\n",
    "# Estimamos los parámetros muestrales (la media y desviación estándar)\n",
    "loc, scale = stats.norm.fit(dataset_pf_identity_mundo)\n",
    "\n",
    "# Creamos una distribución normal\n",
    "norm_dist = stats.norm(loc, scale)\n",
    "\n",
    "# Generamos una muestra\n",
    "sample = norm_dist.rvs(1000)\n",
    "\n",
    "# La graficamos\n",
    "seaborn.distplot(sample, label='Distribución normal')\n",
    "seaborn.distplot(dataset_pf_identity_mundo, label='pf_identity_mundo')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos armar el gráfico QQ. Usamos dos muestras del mismo tamaño, y calculamos los cuartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos 20 puntos percentiles igualmente distribuidos entre 0 y 100.\n",
    "percs = numpy.linspace(0,100,21)\n",
    "\n",
    "qn_sample = numpy.percentile(dataset_pf_identity_mundo, percs)\n",
    "qn_norm_dist = numpy.percentile(norm_dist.rvs(len(dataset_pf_identity_mundo)), percs)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "seaborn.regplot(x=qn_sample, y=qn_norm_dist)\n",
    "\n",
    "plt.xlabel('Percentiles de la muestra de pf_identity_mundo')\n",
    "plt.ylabel('Percentiles de la distribución \\n normal estimada')\n",
    "plt.title('Gráfico QQ de la distribución de pf_identity_mundo y una distribución normal')\n",
    "\n",
    "x = numpy.linspace(numpy.min((qn_sample.min(), qn_norm_dist.min())), numpy.max((qn_sample.max(),qn_norm_dist.max())))\n",
    "plt.plot(x,x, color=\"#FF5964\", ls=\"--\")\n",
    "\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Gráfico QQ de las distribuciones de pf_identity en America Latina y el Caribe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero sacamos los parametros de la media y desviación estándar, para luego hacer una muestra aleatoria que si tenga una distribución normal que vamos a usar como comparador en el gráfico QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Definimos la variable para el dataset que vamos a utilizar\n",
    "dataset_pf_identity_latin_america = dataset[dataset['region'] == 'Latin America & the Caribbean']['pf_identity'].dropna()\n",
    "\n",
    "# Estimamos los parámetros muestrales (la media y desviación estándar)\n",
    "loc, scale = stats.norm.fit(dataset_pf_identity_latin_america)\n",
    "\n",
    "# Creamos una distribución normal\n",
    "norm_dist = stats.norm(loc, scale)\n",
    "\n",
    "# Generamos una muestra\n",
    "sample = norm_dist.rvs(1000)\n",
    "\n",
    "# La graficamos\n",
    "seaborn.distplot(sample, label='Distribución normal')\n",
    "seaborn.distplot(dataset_pf_identity_latin_america, label='pf_identity_latin_america')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos armar el gráfico QQ. Usamos dos muestras del mismo tamaño, y calculamos los cuartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos 20 puntos percentiles igualmente distribuidos entre 0 y 100.\n",
    "percs = numpy.linspace(0,100,21)\n",
    "\n",
    "qn_sample = numpy.percentile(dataset_pf_identity_latin_america, percs)\n",
    "qn_norm_dist = numpy.percentile(norm_dist.rvs(len(dataset_pf_identity_latin_america)), percs)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "seaborn.regplot(x=qn_sample, y=qn_norm_dist)\n",
    "\n",
    "plt.xlabel('Percentiles de la muestra de pf_identity_latin_america')\n",
    "plt.ylabel('Percentiles de la distribución \\n normal estimada')\n",
    "plt.title('Gráfico QQ de la distribución de pf_identity_latin_america y una distribución normal')\n",
    "\n",
    "x = numpy.linspace(numpy.min((qn_sample.min(), qn_norm_dist.min())), numpy.max((qn_sample.max(),qn_norm_dist.max())))\n",
    "plt.plot(x,x, color=\"#FF5964\", ls=\"--\")\n",
    "\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Gráfico QQ de las distribuciones de hf_score en América Latina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero sacamos los parametros de la media y desviación estándar, para luego hacer una muestra aleatoria que si tenga una distribución normal que vamos a usar como comparador en el gráfico QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Definimos la variable para el dataset que vamos a utilizar\n",
    "dataset_hf_score_latin_america = dataset[dataset['region'] == 'Latin America & the Caribbean']['hf_score'].dropna()\n",
    "\n",
    "# Estimamos los parámetros muestrales (la media y desviación estándar)\n",
    "loc, scale = stats.norm.fit(dataset_hf_score_latin_america)\n",
    "\n",
    "# Creamos una distribución normal\n",
    "norm_dist = stats.norm(loc, scale)\n",
    "\n",
    "# Generamos una muestra\n",
    "sample = norm_dist.rvs(1000)\n",
    "\n",
    "# La graficamos\n",
    "seaborn.distplot(sample, label='Distribución normal')\n",
    "seaborn.distplot(dataset_hf_score_latin_america, label='hf_score_latin_america')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos armar el gráfico QQ. Usamos dos muestras del mismo tamaño, y calculamos los cuartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos 20 puntos percentiles igualmente distribuidos entre 0 y 100.\n",
    "percs = numpy.linspace(0,100,21)\n",
    "\n",
    "qn_sample = numpy.percentile(dataset_hf_score_latin_america, percs)\n",
    "qn_norm_dist = numpy.percentile(norm_dist.rvs(len(dataset_hf_score_latin_america)), percs)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "seaborn.regplot(x=qn_sample, y=qn_norm_dist)\n",
    "\n",
    "plt.xlabel('Percentiles de la muestra de hf_score_america_latina')\n",
    "plt.ylabel('Percentiles de la distribución \\n normal estimada')\n",
    "plt.title('Gráfico QQ de la distribución de hf_score_latin_america y una distribución normal')\n",
    "\n",
    "x = numpy.linspace(numpy.min((qn_sample.min(), qn_norm_dist.min())), numpy.max((qn_sample.max(),qn_norm_dist.max())))\n",
    "plt.plot(x,x, color=\"#FF5964\", ls=\"--\")\n",
    "\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Gráfico QQ de las distribuciones de hf_score en el mundo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero sacamos los parametros de la media y desviación estándar, para luego hacer una muestra aleatoria que si tenga una distribución normal que vamos a usar como comparador en el gráfico QQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Definimos la variable para el dataset que vamos a utilizar\n",
    "dataset_hf_score_mundo = dataset['hf_score'].dropna()\n",
    "\n",
    "# Estimamos los parámetros muestrales (la media y desviación estándar)\n",
    "loc, scale = stats.norm.fit(dataset_hf_score_mundo)\n",
    "\n",
    "# Creamos una distribución normal\n",
    "norm_dist = stats.norm(loc, scale)\n",
    "\n",
    "# Generamos una muestra\n",
    "sample = norm_dist.rvs(1000)\n",
    "\n",
    "# La graficamos\n",
    "seaborn.distplot(sample, label='Distribución normal')\n",
    "seaborn.distplot(dataset_hf_score_mundo, label='hf_score_mundo')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos armar el gráfico QQ. Usamos dos muestras del mismo tamaño, y calculamos los cuartiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos 20 puntos percentiles igualmente distribuidos entre 0 y 100.\n",
    "percs = numpy.linspace(0,100,21)\n",
    "\n",
    "qn_sample = numpy.percentile(dataset_hf_score_mundo, percs)\n",
    "qn_norm_dist = numpy.percentile(norm_dist.rvs(len(dataset_hf_score_mundo)), percs)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "seaborn.regplot(x=qn_sample, y=qn_norm_dist)\n",
    "\n",
    "plt.xlabel('Percentiles de la muestra de hf_score_mundo')\n",
    "plt.ylabel('Percentiles de la distribución \\n normal estimada')\n",
    "plt.title('Gráfico QQ de la distribución de hf_score_mundo y una distribución normal')\n",
    "\n",
    "x = numpy.linspace(numpy.min((qn_sample.min(), qn_norm_dist.min())), numpy.max((qn_sample.max(),qn_norm_dist.max())))\n",
    "plt.plot(x,x, color=\"#FF5964\", ls=\"--\")\n",
    "\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.1 Skew/Curtosis pf_identity en América Latina y el mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_pf_identity_latin_america = dataset[dataset['region'] == 'Latin America & the Caribbean']['pf_identity'].dropna()\n",
    "dataset_pf_identity_mundo = dataset['pf_identity'].dropna()\n",
    "\n",
    "# Skew\n",
    "skew_pf_identity_latin_america = stats.skew (dataset_pf_identity_latin_america)\n",
    "skew_pf_identity_mundo = stats.skew (dataset_pf_identity_mundo)\n",
    "\n",
    "# Curtosis\n",
    "kurtosis_pf_identity_latin_america = stats.kurtosis(dataset_pf_identity_latin_america)\n",
    "kurtosis_pf_identity_mundo = stats.kurtosis(dataset_pf_identity_mundo)\n",
    "\n",
    "# Impresión de resultados\n",
    "print ('pf_identity en América Latina' , \"\\n\")\n",
    "print ('Skew : ' , skew_pf_identity_latin_america)\n",
    "print ('Curtosis : ' , kurtosis_pf_identity_latin_america)\n",
    "print ('---------------' + \"\\n\")\n",
    "\n",
    "print ('pf_identity en el mundo' , \"\\n\")\n",
    "print ('Skew : ' , skew_pf_identity_mundo)\n",
    "print ('Curtosis: ' , kurtosis_pf_identity_mundo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis\n",
    "\n",
    "Para pf_identity tanto en America Latina como en el mundo, indican\n",
    "\n",
    "* skew negativo : la cola de distribución es asimetrica hacia la izquiera\n",
    "* curtosis negativa  : hay una menor concentración de datos en torno a la media, excepto en America Latina que tiende a 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 Skew hf_score en América Latina y el mundo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "dataset_hf_score_latin_america = dataset[dataset['region'] == 'Latin America & the Caribbean']['hf_score'].dropna()\n",
    "dataset_hf_score_mundo = dataset['hf_score'].dropna()\n",
    "\n",
    "# Skew\n",
    "skew_hf_score_latin_america = stats.skew (dataset_hf_score_latin_america)\n",
    "skew_hf_score_mundo = stats.skew (dataset_hf_score_mundo)\n",
    "\n",
    "# Curtosis\n",
    "kurtosis_hf_score_latin_america = stats.kurtosis(dataset_hf_score_latin_america)\n",
    "kurtosis_hf_score_mundo = stats.kurtosis(dataset_hf_score_mundo)\n",
    "\n",
    "# Impresión de resultados\n",
    "print ('hf_score en América Latina' , \"\\n\")\n",
    "print ('Skew : ' , skew_hf_score_latin_america)\n",
    "print ('Curtosis : ' , kurtosis_hf_score_latin_america)\n",
    "print ('---------------' + \"\\n\")\n",
    "\n",
    "print ('hf_score en el mundo' , \"\\n\")\n",
    "print ('Skew : ' , skew_hf_score_mundo)\n",
    "print ('Curtosis: ' , kurtosis_hf_score_mundo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analisis\n",
    "\n",
    "Para hf_score tanto en America Latina como en el mundo, indican\n",
    "\n",
    "* skew negativo : la cola de distribución es asimetrica hacia la izquiera\n",
    "* curtosis  : Un valor de 5.3 nos indica una mayor concentración en el medio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Correlaciones\n",
    "\n",
    "En este ejercicio queremos responder a las preguntas\n",
    "\n",
    "* Las libertades sociales y económicas, ¿van siempre de la mano?\n",
    "* ¿Cómo se relacionan ambas con las libertades individuales y respectivas a las relaciones personales?\n",
    "\n",
    "Para ello, analizaremos las correlaciones entre las variables pf_identity, hf_score y ef_score. \n",
    "\n",
    "Como pf_indentity contribuye al cálculo de hf_score y ef_score, esperamos hallar algún grado de correlación. Sin embargo, queremos medir qué tanta correlación. \n",
    "\n",
    "1. ¿Qué conclusiones puede sacar de un gráfico pairplot de estas tres variables? ¿Es adecuado para los valores de pf_identity? ¿Por qué?\n",
    "2. Graficar la correlación entre pf_identity y hf_score; y entre pf_identity y ef_score. Analizar el resultado, ¿se pueden sacar conclusiones? Tengan en cuenta que como pf_identity es el resultado de un promedio, sólo toma algunos valores. Es, en efecto, discreta.\n",
    "3. Calcular algún coeficiente de correlación adecuado entre los dos pares de variables, dependiendo de la cantidad de datos, el tipo de datos y la distribución de los mismo. Algunas opciones son: coeficiente de pearson, coeficiente de spearman, coeficientes de tau y de kendall. Interpretar los resultados y justificar si las variables están correlacionadas o no. \n",
    "4. [Opcional] Analizar la correlación entre la region y el hf_score (y/o el ef_score); y entre la region y el pf_identity. Considerar que como la variable *region* es ordinal, debe utilizarse algún tipo de test. Explicar cuáles son los requisitos necesarios para la aplicación de ese test. (Si no se cumplieran, se pueden agregar algunos datos para generar más registros)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Graficar con pairplot las siguientes variables : pf_identity, hf_score y ef_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaramos las variables a usar\n",
    "dataset_latin_america = dataset[dataset['region'] == 'Latin America & the Caribbean']\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "seaborn.pairplot(dataset_latin_america, vars=['pf_identity', 'hf_score', 'ef_score'], markers='+')\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Graficar la correlación entre pf_identity y hf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "seaborn.scatterplot(data=dataset, x='pf_identity', y='hf_score')\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Graficar la correlación entre pf_identity y ef_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "seaborn.scatterplot(data=dataset, x='pf_identity', y='ef_score')\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calcular algún coeficiente de correlación adecuado entre los dos pares de variables\n",
    "\n",
    "Dependiendo de la cantidad de datos, el tipo de datos y la distribución de los mismo. \n",
    "\n",
    "Algunas opciones son: \n",
    "\n",
    "* coeficiente de pearson\n",
    "* coeficiente de spearman\n",
    "* coeficientes de tau\n",
    "* coeficientes de kendall\n",
    "\n",
    "Interpretar los resultados y justificar si las variables están correlacionadas o no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculo de covarianza \n",
    "\n",
    "covarianza_pf_identity_hf_score = numpy.cov(dataset['pf_identity'].dropna(), dataset['hf_score'].dropna())[0][1]\n",
    "covarianza_pf_identity_ef_score = numpy.cov(dataset['pf_identity'].dropna(), dataset['ef_score'].dropna())[0][1]\n",
    "coeficiente_pf_identity_hf_score = dataset.corr(method=\"pearson\")[\"pf_identity\"][\"hf_score\"]\n",
    "coeficiente_pf_identity_ef_score = dataset.corr(method=\"pearson\")[\"pf_identity\"][\"ef_score\"]\n",
    "\n",
    "# Impresión de resultados\n",
    "\n",
    "print ('\\n'+'Covarianza'+'\\n')\n",
    "print ('Covarianza : pf_identity - hf_score: ', covarianza_pf_identity_hf_score )\n",
    "print ('Covarianza : pf_identity - ef_score: ', covarianza_pf_identity_ef_score )\n",
    "print ('\\n'+'Coeficiente'+'\\n')\n",
    "print ('Coeficiente de Pearson : pf_identity - hf_score: ', coeficiente_pf_identity_hf_score )\n",
    "print ('Coeficiente de Pearson : pf_identity - ef_score: ', coeficiente_pf_identity_ef_score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretación\n",
    "\n",
    "##### Covarianzas\n",
    "\n",
    "Dadas covarianzas positivas podemos decir que existe cierta relación entre los dos pares de variables. Siendo el par pf_identity - hf_score quien más fuerte relación parece mostrar\n",
    "\n",
    "##### Coeficiente\n",
    "\n",
    "Para ambos pares los coeficientes son mayores que cero, por lo tanto existe una correlación positiva"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
